{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e90fbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input arrays are (3500, 2800)\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pydicom as dicom\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5f4bd899",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Positional encoding class\n",
    "#May want to scrap this for a learnable positional encoding model as opposed to sinusoidal \n",
    "class Positional_Encoding(nn.Module):\n",
    "    def __init__(self,data,dropout=0.1,n = 10000):\n",
    "        super(Positional_Encoding, self).__init__()\n",
    "        self.embedded_dim, self.position = data.shape\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.positional_matrix = torch.zeros(self.embedded_dim,self.position)\n",
    "\n",
    "        for pos in range(self.position):\n",
    "            for i in range(int(self.embedded_dim/2)):\n",
    "                denom = pow(n, 2*i/self.embedded_dim) \n",
    "                self.positional_matrix[2*i,pos] = np.sin(pos/denom)\n",
    "                self.positional_matrix[2*i+1,pos] = np.cos(pos/denom)     \n",
    "    \n",
    "    def forward(self,data):\n",
    "#         print(f'Data shape: {data.shape}')\n",
    "#         print(f'positional_matrix shape: {self.positional_matrix.shape}')\n",
    "        self.summer_matrix = data + self.positional_matrix\n",
    "        self.summer_matrix = self.dropout(self.summer_matrix)\n",
    "\n",
    "        return self.summer_matrix, self.positional_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "695ab58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply conv layer to a (500, 400) subset of each scan \n",
    "#TODO max pool is necessary \n",
    "class convlayer(nn.Module):\n",
    "    def __init__(self, num_patch: int = 49):\n",
    "        super(convlayer, self).__init__()\n",
    "        n = num_patch\n",
    "#         self.conv2d_1 = nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size = 13, stride = 1)\n",
    "        self.conv2d_1 = nn.Conv2d(in_channels = n*1, out_channels = n *8, kernel_size = 13, stride = 1, groups = n)\n",
    "        \n",
    "        self.pooling2d_1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv2d_2 = nn.Conv2d(in_channels = n*8, out_channels = n*16, kernel_size = 11, stride = 1, groups = n)\n",
    "\n",
    "        self.pooling2d_2 = nn.MaxPool2d(2)\n",
    "        \n",
    "#         self.conv2d_3 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 9, stride = 1, groups = n)\n",
    "        self.conv2d_3 = nn.Conv2d(in_channels = n*16, out_channels = n*32, kernel_size = 9, stride = 1, groups = n)\n",
    "\n",
    "        \n",
    "#         self.conv2d_4 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 7, stride = 1, groups = n)\n",
    "        self.conv2d_4 = nn.Conv2d(in_channels = n*32, out_channels = n*32, kernel_size = 7, stride = 1, groups = n)\n",
    "\n",
    "        self.pooling2d_3 = nn.MaxPool2d(2)\n",
    "        \n",
    "#         self.conv2d_5 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 5, stride = 1, groups = n)\n",
    "        self.conv2d_5 = nn.Conv2d(in_channels = n*32, out_channels = n*64, kernel_size = 5, stride = 1, groups = n)\n",
    "\n",
    "\n",
    "        \n",
    "        self.dnn = nn.Linear(105280,256)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        \n",
    "    def forward(self, tensor):\n",
    "        x = self.conv2d_1(tensor)\n",
    "        x = self.relu(x)\n",
    "        x = self.pooling2d_1(x)\n",
    "        \n",
    "        x = self.conv2d_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pooling2d_2(x)\n",
    "        \n",
    "        x = self.conv2d_3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2d_4(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.pooling2d_3(x)\n",
    "        \n",
    "        x = self.conv2d_5(x)\n",
    "        x = self.relu(x)\n",
    "        #print(x.shape)\n",
    "        x = self.flatten(x)\n",
    "        x = torch.reshape(x,(105280,1))\n",
    "        x = self.dnn(x.T)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba790ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init for \n",
    "#  \n",
    "\n",
    "def forward():\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af282c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 500, 400])\n"
     ]
    }
   ],
   "source": [
    "sample = torch.zeros(1,500,400)\n",
    "print(sample.shape)\n",
    "model = convlayer()\n",
    "x = model.forward(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3321feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test with one image \n",
    "ds = dicom.dcmread('2ddfad7286c2b016931ceccd1e2c7bbc copy.dicom')\n",
    "info = ds.pixel_array[:3500,:]\n",
    "list_of_patches = []\n",
    "patches_matrix = torch.zeros(49,500,400)\n",
    "for i in range(7):\n",
    "    x_axis = [i*500,(i+1)*500]\n",
    "    for j in range(7):\n",
    "        y_axis = [j*400,(j+1)*400]\n",
    "        correct_patch = np.ascontiguousarray(info[x_axis[0]:x_axis[1],y_axis[0]:y_axis[1]],dtype = np.float32)\n",
    "        tensor = torch.from_numpy(correct_patch)\n",
    "#         list_of_patches.append(tensor)\n",
    "        loc = 7*i + j\n",
    "        patches_matrix[loc,:,:] = tensor\n",
    "# print(len(list_of_patches))\n",
    "# print(patches_matrix.shape[0])\n",
    "# print(list_of_patches[1])\n",
    "# print(patches_matrix[1,:,:])\n",
    "embedded_patches = []\n",
    "embedded_matrix = torch.zeros(256,49)\n",
    "for i in range(patches_matrix.shape[0]):\n",
    "    patch = patches_matrix[i,:,:]\n",
    "#     print(patch.shape)\n",
    "    model = convlayer()\n",
    "    patch = patch[None,:,:]\n",
    "    #print(patch.shape)\n",
    "    x = model.forward(patch)\n",
    "#     embedded_patches.append(x.T)\n",
    "#     print(x)\n",
    "    embedded_matrix[:,i] = x\n",
    "# Learnable embedding\n",
    "learned_embedding_vec = nn.Parameter(torch.zeros(256,1))\n",
    "#nn.Parameter adds it to model paramter so that we can backprop through it \n",
    "embedded_matrix = torch.hstack((learned_embedding_vec,embedded_matrix))\n",
    "\n",
    "#     print(embedded_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd08b944",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "positional = Positional_Encoding(embedded_matrix)\n",
    "summer, pos = positional.forward(embedded_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5e4017bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full Embedding class that takes in data name for individual image and outputs positional embedding where each column\n",
    "#vector represents a positionally-embedded patch except for the very first column vector, which is a learnt \n",
    "#classification token \n",
    "#TODO reimplement vectorized patching via pytorch link \n",
    "class embedding_block(nn.Module):\n",
    "    #Data in this sense is the image that has not been translated into an array\n",
    "    #Want to set x_con to 3500\n",
    "    def __init__(self, x_amount = 7, y_amount = 7, x_con = 3518, y_con = 2800):\n",
    "        super(embedding_block, self).__init__()\n",
    "        \n",
    "        assert(x_con % x_amount == 0)\n",
    "        assert(y_con % y_amount == 0)\n",
    "        self.x_amount = x_amount\n",
    "        self.y_amount = y_amount\n",
    "        self.x_con = x_con\n",
    "        self.y_con = y_con\n",
    "        \n",
    "        \n",
    "        self.amount_of_patches = int(x_amount * y_amount)\n",
    "        self.x_ran = int(x_con / x_amount)\n",
    "        self.y_ran = int(y_con / y_amount)\n",
    "        self.patches_matrix = torch.zeros(self.amount_of_patches,self.x_ran,self.y_ran)\n",
    "        #print(self.patches_matrix.shape)\n",
    "        \n",
    "        #print('here')\n",
    "    \n",
    "    def forward(self, data):\n",
    "        ds = dicom.dcmread(data)\n",
    "        self.info = ds.pixel_array[:self.x_con,:self.y_con]\n",
    "        for i in range(self.x_amount):\n",
    "            x_axis = [i*self.x_ran,(i+1)*self.x_ran]\n",
    "            for j in range(self.y_amount):\n",
    "                y_axis = [j*self.y_ran,(j+1)*self.y_ran]\n",
    "                correct_patch = np.ascontiguousarray(self.info[x_axis[0]:x_axis[1],y_axis[0]:y_axis[1]],\n",
    "                                                     dtype = np.float32)\n",
    "                tensor = torch.from_numpy(correct_patch)\n",
    "                loc = self.x_amount*i + j\n",
    "                self.patches_matrix[loc,:,:] = tensor\n",
    "        \n",
    "        \n",
    "#         self.embedded_matrix = torch.zeros(256,self.amount_of_patches)\n",
    "        self.embedded_matrix = torch.zeros(self.amount_of_patches,256)\n",
    "        for i in range(patches_matrix.shape[0]):\n",
    "            self.patch = self.patches_matrix[i,:,:]\n",
    "            self.model = convlayer()\n",
    "            self.patch = self.patch[None,:,:]\n",
    "            self.x = self.model.forward(patch)\n",
    "            self.embedded_matrix[i,:] = self.x\n",
    "            # Dealing with (RCC, LCC)  and (RMLO, LMLO)\n",
    "            # init 2 conv layers that: \n",
    "            #      - torch.Size([B, 49, 500, 400]) apply some function that applies 49 independent conv \n",
    "            #      layers to dim 1 i.e. clever use of reshape/channels/batch_size/groups\n",
    "            #      - torch.Size([B, 49, 256])\n",
    "            # \n",
    "            # init -> conv_1\n",
    "            # init -> conv_2 \n",
    "            # \n",
    "            # rcc_after_conv = conv_1(RCC)\n",
    "            # lcc_after_conv = conv_1(mirror(LCC))\n",
    "            #      # or do RCC cat mirror(LCC) ==> torch.Size([B*2, 49, 256])\n",
    "            # \n",
    "            # \n",
    "            # conv_2(RMLO)\n",
    "            # conv_2(mirror(LMLO))\n",
    "\n",
    "            \n",
    "#         self.learned_embedding_vec = nn.Parameter(torch.zeros(256,1))\n",
    "        self.learned_embedding_vec = nn.Parameter(torch.zeros(1,256))\n",
    "        #nn.Parameter adds it to model paramter so that we can backprop through it \n",
    "#         self.embedded_matrix = torch.v=hstack((self.learned_embedding_vec,self.embedded_matrix))\n",
    "        self.embedded_matrix = torch.vstack((self.learned_embedding_vec,self.embedded_matrix))\n",
    "#         print(f'self.embedded_matrix shape: {self.embedded_matrix.shape}')\n",
    "        self.positional = Positional_Encoding(self.embedded_matrix)\n",
    "#         print(f'positional_matrix shpae: {self.positional.positional_matrix.shape}')\n",
    "        self.summer, self.pos = self.positional.forward(self.embedded_matrix)\n",
    "        \n",
    "#         print(f'self.summer shape: {self.summer.shape}')\n",
    "        \n",
    "        return self.summer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d331a459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.9350,  1.0103,  ...,  0.0000,  0.5022, -0.5627],\n",
       "        [ 1.1080,  0.5853, -0.4702,  ..., -0.1034, -0.9863, -0.9570],\n",
       "        [-0.0000,  0.7062,  0.0000,  ..., -0.8839, -0.0000,  0.5322],\n",
       "        ...,\n",
       "        [ 1.1170,  1.1192,  1.1094,  ...,  0.0000,  1.0976,  0.0000],\n",
       "        [ 0.0216, -0.0076, -0.0090,  ...,  0.0319,  0.0447,  0.0464],\n",
       "        [ 1.1118,  1.0984,  1.0978,  ...,  1.1012,  1.1039,  1.1173]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = embedding_block(x_con = 3500)\n",
    "x.forward('2ddfad7286c2b016931ceccd1e2c7bbc copy.dicom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "02fb6908",
   "metadata": {},
   "outputs": [],
   "source": [
    "class local_mlp(nn.Module):\n",
    "    def __init__(self, hidden_output = 1024, dropout = .5):\n",
    "        super(local_mlp, self).__init__()\n",
    "        self.fnn1 = nn.Linear(256,hidden_output)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.fnn2 = nn.Linear(hidden_output,256)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    def forward(self, data):\n",
    "        self.x = self.fnn1(data)\n",
    "        self.x = self.gelu(self.x)\n",
    "        self.x = self.dropout1(self.x)\n",
    "        self.x = self.fnn2(self.x)\n",
    "        self.x = self.gelu(self.x)\n",
    "        self.x = self.dropout2(self.x)\n",
    "        \n",
    "        return self.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e9f808a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class local_encoder_block(nn.Module):\n",
    "    def __init__(self, data_shape = (50,256), hidden_output_fnn1 = 1024, dropout = .5):\n",
    "        super(local_encoder_block, self).__init__()\n",
    "        self.data_shape = data_shape\n",
    "        self.ln1 = nn.LayerNorm(data_shape)\n",
    "        self.ln2 = nn.LayerNorm(data_shape)\n",
    "        \n",
    "        self.attention = nn.MultiheadAttention(256,16)\n",
    "        self.mlp = local_mlp(hidden_output = hidden_output_fnn1, dropout = dropout)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        #i = 0\n",
    "        #print(i)\n",
    "        if data.shape == (256,50):\n",
    "            print('in here')\n",
    "            data = data.T\n",
    "        self.x = self.ln1(data)\n",
    "        self.att_out, self.att_out_weights = self.attention(query = self.x,key = self.x,value = self.x)\n",
    "        #print(self.att_out.shape)\n",
    "        self.x_tilda = self.att_out + data\n",
    "        #print(self.x_tilda.shape)\n",
    "        self.x_second = self.ln2(self.x_tilda)\n",
    "        self.x_second = self.mlp.forward(self.x_second)\n",
    "        #print(self.x_second.shape)\n",
    "        self.x_second = self.x_second + self.x_tilda\n",
    "        \n",
    "        #i += 1\n",
    "        return self.x_second  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "763cb569",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visual_Transformer(nn.Module):\n",
    "    #embedding parameters, local encoder parameters\n",
    "    def __init__(self, x_amount = 7, y_amount = 7, x_con = 3518, y_con = 2800,\n",
    "              data_shape = (50,256), hidden_output_fnn = 1024, dropout = .5,\n",
    "              number_of_layers = 10):\n",
    "        super(Visual_Transformer, self).__init__()\n",
    "        self.embedding_block = embedding_block(x_amount = x_amount, y_amount = y_amount, x_con = x_con, y_con = y_con)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(number_of_layers):\n",
    "            self.blks.add_module(f'{i}', local_encoder_block(data_shape = data_shape))\n",
    "            \n",
    "    def forward(self,data):\n",
    "        self.x = self.embedding_block(data)\n",
    "        print(self.x.shape)\n",
    "        i = 0 \n",
    "        for blk in self.blks:\n",
    "            print(f'This is {i} run')\n",
    "            i += 1\n",
    "            self.x = blk(self.x) \n",
    "        return self.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68b3a15",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = Visual_Transformer(x_con = 3500)\n",
    "output = x.forward('2ddfad7286c2b016931ceccd1e2c7bbc copy.dicom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "21491b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-3.4374e-01,  7.1017e+00,  1.9626e+01, -2.8022e-01, -1.1748e+00,\n",
      "        -1.4898e+00, -5.6181e-01, -5.4902e+00, -8.9077e+00, -1.0441e+01,\n",
      "        -8.9423e-01, -1.5933e+00, -9.3679e-01, -5.5882e-04,  8.7317e-01,\n",
      "        -4.4071e+00,  2.6351e+00, -8.5413e-01, -1.1503e+00, -1.7661e-01,\n",
      "         7.9309e-01,  6.6905e-01,  2.7895e+01,  9.5001e+00, -1.2156e+00,\n",
      "        -4.0845e-01,  5.4589e-01,  8.1266e-01,  7.0526e-02, -3.0588e+00,\n",
      "         1.1913e+01, -1.2253e+00, -2.4221e-01,  9.2028e-01,  3.7052e-01,\n",
      "        -8.5558e-01, -1.3633e+00, -1.1262e+00,  1.3749e-01,  7.8645e-01,\n",
      "        -2.8929e-01, -4.6127e-01, -1.8099e-01, -1.0848e+00, -1.4540e-01,\n",
      "         7.1564e-01,  6.0247e-01, -2.2884e-01, -1.3012e+00, -1.3794e+00],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output.T[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
