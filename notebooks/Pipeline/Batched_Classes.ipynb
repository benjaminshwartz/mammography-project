{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87ced439",
   "metadata": {},
   "source": [
    "NOTE THE UPDATED CLASSES ARE BELOW THESE PREVIOUSLY DEFINED CLASSES.SCROLL DOWN TO LAST MARKDOWN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d8c61d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pydicom as dicom\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8b2c087f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, data, dropout=0.1, n=10000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.batch_size ,self.embedded_dim, self.position = data.shape\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        self.embedded_dim += 1  # adding one to embedded dim to take into account token prepend\n",
    "\n",
    "        self.learned_embedding_vec = nn.Parameter(\n",
    "            torch.zeros(self.batch_size, 1, self.position))\n",
    "\n",
    "        self.positional_matrix = torch.zeros(self.embedded_dim, self.position)\n",
    "\n",
    "        for pos in range(self.position):\n",
    "            for i in range(int(self.embedded_dim/2)):\n",
    "                denom = pow(n, 2*i/self.embedded_dim)\n",
    "                self.positional_matrix[2*i, pos] = np.sin(pos/denom)\n",
    "                self.positional_matrix[2*i+1, pos] = np.cos(pos/denom)\n",
    "                \n",
    "        self.positional_matrix = self.positional_matrix[None,:,:]\n",
    "        self.positional_matrix = self.positional_matrix.tile((self.batch_size,1,1))\n",
    "\n",
    "    def forward(self, data):\n",
    "        data = torch.hstack((self.learned_embedding_vec, data))\n",
    "        summer_matrix = data + self.positional_matrix\n",
    "        summer_matrix = self.dropout(summer_matrix)\n",
    "        print(summer_matrix.shape)\n",
    "\n",
    "        return summer_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d70e0770",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self,batch_size, num_patch: int = 49 ):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        self.num_patch = num_patch\n",
    "        self.batch_size = batch_size\n",
    "        n = num_patch\n",
    "#         self.conv2d_1 = nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size = 13, stride = 1)\n",
    "        self.conv2d_1 = nn.Conv2d(in_channels = n*1, out_channels = n *8, kernel_size = 11, stride = 1, groups = n)\n",
    "        \n",
    "#         self.pooling2d_1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv2d_2 = nn.Conv2d(in_channels = n*8, out_channels = n*16, kernel_size = 9, stride = 1, groups = n)\n",
    "\n",
    "#         self.pooling2d_2 = nn.MaxPool2d(2)\n",
    "        \n",
    "#         self.conv2d_3 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 9, stride = 1, groups = n)\n",
    "        self.conv2d_3 = nn.Conv2d(in_channels = n*16, out_channels = n*32, kernel_size = 7, stride = 1, groups = n)\n",
    "\n",
    "        \n",
    "#         self.conv2d_4 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 7, stride = 1, groups = n)\n",
    "        self.conv2d_4 = nn.Conv2d(in_channels = n*32, out_channels = n*32, kernel_size = 5, stride = 1, groups = n)\n",
    "\n",
    "#         self.pooling2d_3 = nn.MaxPool2d(2)\n",
    "        \n",
    "#         self.conv2d_5 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 5, stride = 1, groups = n)\n",
    "        self.conv2d_5 = nn.Conv2d(in_channels = n*32, out_channels = n*64, kernel_size = 3, stride = 1, groups = n)\n",
    "\n",
    "\n",
    "        \n",
    "        self.dnn = nn.Linear(256,128)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "\n",
    "    def forward(self, tensor):\n",
    "        # print('IN FORWARD OF CONV LAYER')\n",
    "        tensor = tensor[:,]\n",
    "        print(tensor.shape)\n",
    "        # print(f'THIS IS THE SHAPE OF THE TENSOR: {tensor.shape}')\n",
    "        x = self.conv2d_1(tensor)\n",
    "        x = self.relu(x)\n",
    "#         x = self.pooling2d_1(x)\n",
    "        print(x.shape)\n",
    "\n",
    "        x = self.conv2d_2(x)\n",
    "        x = self.relu(x)\n",
    "#         x = self.pooling2d_2(x)\n",
    "\n",
    "        x = self.conv2d_3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2d_4(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "#         x = self.pooling2d_3(x)\n",
    "\n",
    "        x = self.conv2d_5(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        print(x.shape)\n",
    "        val = x.shape[1]/self.num_patch\n",
    "        x = torch.reshape(x, (self.batch_size, self.num_patch, int(val)))\n",
    "        print(val)\n",
    "        x = self.dnn(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10584dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "cn = ConvLayer(1,196)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "71a34040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 196, 32, 32])\n",
      "torch.Size([1, 1568, 22, 22])\n",
      "torch.Size([1, 50176])\n",
      "256.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0272,  0.0099,  0.0711,  ...,  0.0440, -0.0022, -0.0439],\n",
       "         [ 0.0495, -0.0272,  0.0700,  ...,  0.0488,  0.0108, -0.0354],\n",
       "         [ 0.0252, -0.0151,  0.0641,  ...,  0.0577, -0.0092, -0.0430],\n",
       "         ...,\n",
       "         [ 0.0538, -0.0033,  0.0603,  ...,  0.0729,  0.0073, -0.0402],\n",
       "         [ 0.0397, -0.0339,  0.0609,  ...,  0.0360, -0.0091, -0.0219],\n",
       "         [ 0.0313, -0.0141,  0.0565,  ...,  0.0606,  0.0004, -0.0433]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(1,196,32,32)\n",
    "cn.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "787dfc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingBlock(nn.Module):\n",
    "    # Data in this sense is the image that has not been translated into an array\n",
    "    # Want to set x_con to 3500\n",
    "    def __init__(self, batch, x_amount=7, y_amount=7, x_con=3500, y_con=2800):\n",
    "        super(EmbeddingBlock, self).__init__()\n",
    "\n",
    "        assert (x_con % x_amount == 0)\n",
    "#         print(y_con)\n",
    "#         print(y_amount)\n",
    "        assert (y_con % y_amount == 0)\n",
    "        self.x_amount = x_amount\n",
    "        self.y_amount = y_amount\n",
    "        self.x_con = x_con\n",
    "        self.y_con = y_con\n",
    "\n",
    "        self.amount_of_patches = int(x_amount * y_amount)\n",
    "        self.x_ran = int(x_con / x_amount)\n",
    "        self.y_ran = int(y_con / y_amount)\n",
    "        self.patches_matrix = torch.zeros(\n",
    "            self.amount_of_patches, self.x_ran, self.y_ran)\n",
    "        \n",
    "        print(self.amount_of_patches)\n",
    "\n",
    "        self.cc_conv = ConvLayer(batch_size = batch, num_patch = self.amount_of_patches)\n",
    "        self.mlo_conv = ConvLayer(batch_size = batch, num_patch = self.amount_of_patches)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # recheck for proper class variables(change self. to strictly local variable)\n",
    "        # print('IN FORWARD OF EMBEDDING BLOCK LAYER')\n",
    "        # Data shape (batch size, num of views, x_length, y_length)\n",
    "        info = data\n",
    "\n",
    "        batch_size = info.shape[0]\n",
    "        \n",
    "\n",
    "        batched_patches = info.unfold(\n",
    "            2, self.x_ran, self.x_ran).unfold(3, self.y_ran, self.y_ran)\n",
    "        batched_patches = torch.reshape(batched_patches,\n",
    "                                        (batch_size, 4, self.amount_of_patches, self.x_ran, self.y_ran))\n",
    "        print(batched_patches.shape)\n",
    "\n",
    "        #Reshape now makes data (batch_size,4,49,500,400)\n",
    "        \n",
    "        \n",
    "        \n",
    "        LCC = batched_patches[:,0]\n",
    "        LMLO = batched_patches[:,1]\n",
    "        RCC = batched_patches[:,2]\n",
    "        RMLO = batched_patches[:,3]\n",
    "\n",
    "        LCC = self.cc_conv.forward(LCC)\n",
    "        RCC = self.cc_conv.forward(RCC)\n",
    "        LMLO = self.mlo_conv.forward(LMLO)\n",
    "        RMLO = self.mlo_conv.forward(RMLO)\n",
    "\n",
    "        pos_encoding_LCC = PositionalEncoding(LCC)\n",
    "        pos_encoding_RCC = PositionalEncoding(RCC)\n",
    "        pos_encoding_LMLO = PositionalEncoding(LMLO)\n",
    "        pos_encoding_RMLO = PositionalEncoding(RMLO)\n",
    "\n",
    "        summer_LCC = pos_encoding_LCC.forward(LCC)\n",
    "        summer_RCC = pos_encoding_RCC.forward(RCC)\n",
    "        summer_LMLO = pos_encoding_LMLO.forward(LMLO)\n",
    "        summer_RMLO = pos_encoding_RMLO.forward(RMLO)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        batched_positional_encoding = torch.zeros(batch_size, 4, summer_LCC.shape[1], summer_LCC.shape[2])\n",
    "        print(f'summer shape :{summer_LCC.shape}')\n",
    "#         print(batched_positional_encoding[:,0].shape)\n",
    "\n",
    "        batched_positional_encoding[:,0] = summer_LCC\n",
    "        batched_positional_encoding[:,1] = summer_LMLO\n",
    "        batched_positional_encoding[:,2] = summer_RCC\n",
    "        batched_positional_encoding[:,3] = summer_RMLO\n",
    "\n",
    "        return batched_positional_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a18359db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_layer, hidden_output=1024, dropout=.5):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fnn1 = nn.Linear(input_layer, hidden_output)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.fnn2 = nn.Linear(hidden_output, input_layer)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # print('IN FORWARD OF MLP LAYER')\n",
    "        x = self.fnn1(data)\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fnn2(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "68b5ed4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalEncoderBlock(nn.Module):\n",
    "    def __init__(self, data_shape, hidden_output_fnn1=1024, dropout=.5):\n",
    "        super(LocalEncoderBlock, self).__init__()\n",
    "        self.data_shape = data_shape\n",
    "        # Layer norm over the H and W of each image\n",
    "        self.batch_size = 10\n",
    "#         print([data_shape[2], data_shape[3]])\n",
    "        self.ln1 = nn.LayerNorm([data_shape[2], data_shape[3]])\n",
    "        self.ln2 = nn.LayerNorm([data_shape[2], data_shape[3]])\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=data_shape[3], num_heads=16, batch_first=True)\n",
    "        self.mlp_0 = MLP(input_layer = data_shape[3],\n",
    "            hidden_output=hidden_output_fnn1, dropout=dropout)\n",
    "        self.mlp_1 = MLP(input_layer = data_shape[3],\n",
    "            hidden_output=hidden_output_fnn1, dropout=dropout)\n",
    "        self.mlp_2 = MLP(input_layer = data_shape[3],\n",
    "            hidden_output=hidden_output_fnn1, dropout=dropout)\n",
    "        self.mlp_3 = MLP(input_layer = data_shape[3],\n",
    "            hidden_output=hidden_output_fnn1, dropout=dropout)\n",
    "    \n",
    "\n",
    "    \n",
    "    def forward(self, data):\n",
    "        # print('IN FORWARD OF LOCALENCODERBLOCK LAYER')\n",
    "        if data.shape == (4, 256, 50):\n",
    "            # print('in here')\n",
    "            data = data.T\n",
    "#         x = self.ln1(data)\n",
    "#         att_out, att_out_weights = self.attention(\n",
    "#             query=x, key=x, value=x)\n",
    "#         x_tilda = att_out + data\n",
    "#         x_second = self.ln2(x_tilda)\n",
    "        x_tilda_matrix = torch.zeros(self.data_shape)\n",
    "        attn_0, y = self.helper_thing(data[:,0])\n",
    "        x_tilda_matrix[:,0] = y\n",
    "        attn_1, y = self.helper_thing(data[:,1])\n",
    "        x_tilda_matrix[:,1] = y\n",
    "        attn_2, y = self.helper_thing(data[:,2])\n",
    "        x_tilda_matrix[:,2] = y\n",
    "        attn_3, y = self.helper_thing(data[:,3])\n",
    "        x_tilda_matrix[:,3] = y\n",
    "        \n",
    "        dnn_output = torch.zeros(self.data_shape)\n",
    "        dnn_output[:,0] = self.mlp_0.forward(attn_0)\n",
    "        dnn_output[:,1] = self.mlp_1.forward(attn_1)\n",
    "        dnn_output[:,2] = self.mlp_2.forward(attn_2)\n",
    "        dnn_output[:,3] = self.mlp_3.forward(attn_3)\n",
    "        x_second = dnn_output + x_tilda_matrix\n",
    "\n",
    "        return x_second\n",
    "    \n",
    "    def helper_thing(self,data):\n",
    "    #Data should be of shape (batch_size, 256, 50)\n",
    "        x = self.ln1(data)\n",
    "#         print(f'x.shape: {x.shape}')\n",
    "        att_out, att_out_weights = self.attention(\n",
    "                query=x, key=x, value=x)\n",
    "        x_tilda = att_out + data\n",
    "        x_second = self.ln2(x_tilda)\n",
    "#         print(x_tilda.shape)\n",
    "        \n",
    "        return x_second, x_tilda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ff5880cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualTransformer(nn.Module):\n",
    "    # embedding parameters, local encoder parameters\n",
    "    def __init__(self, x_amount=7, y_amount=7, x_con=3500, y_con=2800,\n",
    "                 data_shape=(10,4, 50, 256), hidden_output_fnn=1024, dropout=.5,\n",
    "                 number_of_layers=10):\n",
    "        super(VisualTransformer, self).__init__()\n",
    "        self.embedding_block = EmbeddingBlock(batch = data_shape[0],\n",
    "            x_amount=x_amount, y_amount=y_amount, x_con=x_con, y_con=y_con)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(number_of_layers):\n",
    "            self.blks.add_module(\n",
    "                f'{i}', LocalEncoderBlock(data_shape=data_shape))\n",
    "\n",
    "    def forward(self, data):\n",
    "        # print('IN FORWARD OF VISUALTRANSFORMER LAYER')\n",
    "        x = self.embedding_block.forward(data)\n",
    "        # print(x.shape)\n",
    "        i = 0\n",
    "        for blk in self.blks:\n",
    "            # print(f'This is {i} local attention run')\n",
    "            i += 1\n",
    "            x = blk(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e6895ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalEncoderBlock(nn.Module):\n",
    "    def __init__(self, data_shape=(10, 200, 256), hidden_output_fnn1=1024, dropout=.5):\n",
    "        super(GlobalEncoderBlock, self).__init__()\n",
    "        self.data_shape = data_shape\n",
    "        self.gln1 = nn.LayerNorm(data_shape)\n",
    "        self.ln2 = nn.LayerNorm(data_shape)\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=data_shape[2], num_heads=16, batch_first=True)\n",
    "        self.mlp = MLP(input_layer = data_shape[2], hidden_output=hidden_output_fnn1, dropout=dropout)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # print('IN FORWARD OF GLOBALENCODERBLOCK LAYER')\n",
    "        x = self.gln1(data)\n",
    "        att_out, att_out_weights = self.attention(query=x, key=x, value=x)\n",
    "        x_tilda = att_out + data\n",
    "        x_second = self.ln2(x_tilda)\n",
    "        dnn_output = self.mlp.forward(x_second)\n",
    "        x_second = dnn_output + x_tilda\n",
    "\n",
    "        return x_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0fc7bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalTransformer(nn.Module):\n",
    "    def __init__(self, x_amount=7, y_amount=7, x_con=3500, y_con=2800,\n",
    "                 data_shape=(10, 4, 50, 256), hidden_output_fnn=1024, dropout=.5,\n",
    "                 number_of_layers=10, num_layers_global=10):\n",
    "        super(GlobalTransformer, self).__init__()\n",
    "        self.data_shape = data_shape\n",
    "        new_data_shape = (data_shape[0], data_shape[1]*data_shape[2], data_shape[3])\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers_global):\n",
    "            self.blks.add_module(\n",
    "                f'{i}', GlobalEncoderBlock(data_shape=new_data_shape))\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # self.class_head = classification_head(input_layer=data_shape[0]*data_shape[2],\n",
    "        #                                       hidden_output_class=512, dropout=.5)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # print('IN FORWARD OF GLOBALTRANSFORMER LAYER')\n",
    "        #x = self.individual_transformer.forward(data)\n",
    "        \n",
    "        shape0, shape1, shape2, shape3 = data.shape\n",
    "        x = torch.reshape(data, (shape0, shape1 * shape2, shape3))\n",
    "        i = 0\n",
    "        for blk in self.blks:\n",
    "            # print(f'This is {i} global attention run')\n",
    "            x = blk(x)\n",
    "            i += 1\n",
    "\n",
    "#         x = torch.squeeze(x)\n",
    "        # print(x.shape)\n",
    "        x = x[:,[0, 1 * shape2, 2 * shape2, 3 * shape2], :]\n",
    "        x = torch.reshape(x, (x.shape[0], x.shape[1]*x.shape[2]))\n",
    "#         print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ffce544",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, input_layer=1024, hidden_output_class=512, dropout=0.5):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.ln1 = nn.LayerNorm(input_layer)\n",
    "        self.fnn1 = nn.Linear(input_layer, hidden_output_class)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln2 = nn.LayerNorm(hidden_output_class)\n",
    "        self.fnn2 = nn.Linear(hidden_output_class, 5)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # print('IN FORWARD OF CLASSIFICATIONHEAD LAYER')\n",
    "        x = self.ln1(data)\n",
    "        x = self.fnn1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.ln2(x)\n",
    "        x = self.fnn2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "d4020b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperModel(nn.Module):\n",
    "    def __init__(self, x_amount=7, y_amount=7, x_con=3500, y_con=2800,\n",
    "                 data_shape=(10, 4, 50, 256), hidden_output_fnn=1024, dropout=.5,\n",
    "                 number_of_layers=10, num_layers_global=10):\n",
    "\n",
    "        super(PaperModel, self).__init__()\n",
    "\n",
    "        self.embedding_block = EmbeddingBlock(batch = data_shape[0],\n",
    "            x_amount=x_amount, y_amount=y_amount, x_con=x_con, y_con=y_con)\n",
    "\n",
    "        self.visual_transformer = VisualTransformer(x_amount, y_amount, x_con, y_con,\n",
    "                                                    data_shape, hidden_output_fnn, dropout,\n",
    "                                                    number_of_layers)\n",
    "\n",
    "        self.global_transformer = GlobalTransformer(x_amount, y_amount, x_con, y_con,\n",
    "                                                    data_shape, hidden_output_fnn, dropout,\n",
    "                                                    number_of_layers, num_layers_global)\n",
    "\n",
    "        self.classification_head_left = ClassificationHead(\n",
    "            input_layer=data_shape[3]*4, hidden_output_class=512, dropout=0.5)\n",
    "\n",
    "        self.classification_head_right = ClassificationHead(\n",
    "            input_layer=data_shape[3]*4, hidden_output_class=512, dropout=0.5)\n",
    "\n",
    "    def forward(self, data):\n",
    "        #X = self.embedding_block(data)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #data = torch.reshape(data, (4,3500,2800))\n",
    "        #here\n",
    "        # print(f'THIS IS THE DATA SHAPE: {data.shape}')\n",
    "\n",
    "        X = self.visual_transformer(data)\n",
    "\n",
    "        X = self.global_transformer(X)\n",
    "        print(f'X SHAPE:{X.shape}')\n",
    "\n",
    "        left_pred = self.classification_head_left(X)\n",
    "        print(f'left shape: {left_pred.shape}')\n",
    "\n",
    "        right_pred = self.classification_head_right(X)\n",
    "        print(f'right shape: {right_pred.shape}')\n",
    "        \n",
    "        final = torch.zeros(left_pred.shape[0],5,2)\n",
    "        final[:,:,0] = left_pred\n",
    "        final[:,:,1] = right_pred\n",
    "        \n",
    "#         final = torch.vstack((left_pred, right_pred))\n",
    "        \n",
    "        # print(f'Finished data classification, returning vectors of shape: {final.shape}')\n",
    "              \n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "91ebc159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196\n",
      "196\n",
      "torch.Size([1, 4, 196, 32, 32])\n",
      "torch.Size([1, 196, 32, 32])\n",
      "torch.Size([1, 1568, 22, 22])\n",
      "torch.Size([1, 50176])\n",
      "256.0\n",
      "torch.Size([1, 196, 32, 32])\n",
      "torch.Size([1, 1568, 22, 22])\n",
      "torch.Size([1, 50176])\n",
      "256.0\n",
      "torch.Size([1, 196, 32, 32])\n",
      "torch.Size([1, 1568, 22, 22])\n",
      "torch.Size([1, 50176])\n",
      "256.0\n",
      "torch.Size([1, 196, 32, 32])\n",
      "torch.Size([1, 1568, 22, 22])\n",
      "torch.Size([1, 50176])\n",
      "256.0\n",
      "torch.Size([1, 197, 128])\n",
      "torch.Size([1, 197, 128])\n",
      "torch.Size([1, 197, 128])\n",
      "torch.Size([1, 197, 128])\n",
      "summer shape :torch.Size([1, 197, 128])\n",
      "X SHAPE:torch.Size([1, 512])\n",
      "left shape: torch.Size([1, 5])\n",
      "right shape: torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "pap = PaperModel(x_amount=14, y_amount=14,x_con=448, y_con=448, data_shape = (1,4,197,128))\n",
    "zeros = torch.zeros(1,4,448,448)\n",
    "x = pap.forward(zeros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcc6c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f32f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "#for batch size of 10 (10,5,2)\n",
    "predicted = torch.zeros(10,5,2)\n",
    "labels = torch.zeros(10,2).long()\n",
    "loss_fn(predicted,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36ed13a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cn = ConvLayer(batch_size = 1,num_patch = 196)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "dd8b5833",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196\n"
     ]
    }
   ],
   "source": [
    "cn = EmbeddingBlock(1, x_amount=14, y_amount=14, x_con=448, y_con=448)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "06eb92d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 196, 32, 32])\n",
      "torch.Size([1, 196, 32, 32])\n",
      "torch.Size([1, 1568, 22, 22])\n",
      "torch.Size([1, 50176])\n",
      "256.0\n",
      "torch.Size([1, 196, 32, 32])\n",
      "torch.Size([1, 1568, 22, 22])\n",
      "torch.Size([1, 50176])\n",
      "256.0\n",
      "torch.Size([1, 196, 32, 32])\n",
      "torch.Size([1, 1568, 22, 22])\n",
      "torch.Size([1, 50176])\n",
      "256.0\n",
      "torch.Size([1, 196, 32, 32])\n",
      "torch.Size([1, 1568, 22, 22])\n",
      "torch.Size([1, 50176])\n",
      "256.0\n",
      "torch.Size([1, 197, 128])\n",
      "torch.Size([1, 197, 128])\n",
      "torch.Size([1, 197, 128])\n",
      "torch.Size([1, 197, 128])\n",
      "summer shape :torch.Size([1, 197, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000,  0.9350,  1.0103,  ..., -0.6845,  0.3667,  1.0807],\n",
       "          [ 1.0737,  0.6363, -0.4064,  ...,  0.7977,  1.0068,  0.2381],\n",
       "          [-0.0240,  0.9198,  1.1506,  ...,  0.7011,  1.0789,  0.6129],\n",
       "          ...,\n",
       "          [-0.0193,  0.0691,  0.0580,  ..., -0.0765, -0.0166,  0.0271],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  1.0290,  1.0851,  1.1230],\n",
       "          [-0.0189,  0.0470,  0.0784,  ..., -0.0711, -0.0323, -0.0031]],\n",
       "\n",
       "         [[ 0.0000,  0.9350,  1.0103,  ..., -0.6845,  0.3667,  1.0807],\n",
       "          [ 1.1568,  0.6859, -0.4502,  ...,  0.8663,  1.0603,  0.0000],\n",
       "          [ 0.0291,  0.9479,  1.0879,  ...,  0.7508,  1.1159,  0.5405],\n",
       "          ...,\n",
       "          [ 0.0366,  0.0555,  0.0177,  ..., -0.0018,  0.0452, -0.0174],\n",
       "          [ 1.1391,  1.1871,  1.1498,  ...,  1.1213,  1.1466,  1.0590],\n",
       "          [ 0.0292,  0.0687,  0.0175,  ...,  0.0061,  0.0206, -0.0418]],\n",
       "\n",
       "         [[ 0.0000,  0.9350,  1.0103,  ..., -0.6845,  0.3667,  1.0807],\n",
       "          [ 1.0737,  0.6363, -0.4064,  ...,  0.7977,  1.0068,  0.2381],\n",
       "          [-0.0240,  0.9198,  1.1506,  ...,  0.7011,  1.0789,  0.6129],\n",
       "          ...,\n",
       "          [-0.0000,  0.0691,  0.0580,  ..., -0.0765, -0.0166,  0.0271],\n",
       "          [ 1.0856,  1.1577,  1.1945,  ...,  0.0000,  1.0851,  1.1230],\n",
       "          [-0.0000,  0.0470,  0.0784,  ..., -0.0000, -0.0323, -0.0031]],\n",
       "\n",
       "         [[ 0.0000,  0.9350,  1.0103,  ..., -0.6845,  0.3667,  0.0000],\n",
       "          [ 1.1568,  0.6859, -0.4502,  ...,  0.8663,  1.0603,  0.2062],\n",
       "          [ 0.0291,  0.0000,  1.0879,  ...,  0.0000,  1.1159,  0.5405],\n",
       "          ...,\n",
       "          [ 0.0366,  0.0555,  0.0177,  ..., -0.0018,  0.0452, -0.0174],\n",
       "          [ 1.1391,  1.1871,  1.1498,  ...,  0.0000,  1.1466,  0.0000],\n",
       "          [ 0.0292,  0.0687,  0.0175,  ...,  0.0061,  0.0206, -0.0000]]]],\n",
       "       grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros(1,4,448,448)\n",
    "cn.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9e3ee22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.zeros(10)\n",
    "y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016b4e10",
   "metadata": {},
   "source": [
    "THESE ARE THE NEW CLASSES DEFINED IN THE NEW SRC CODE. REFERENCE THESE CLASSES AND THESE CLASSES ONLY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "93b32261",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, data, dropout=0.1, n=10000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.batch_size, self.embedded_dim, self.position = data.shape\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # device = 'cpu'\n",
    "        # if torch.cuda.is_available():\n",
    "        #     device = 'cuda'\n",
    "\n",
    "        device = data.get_device()\n",
    "\n",
    "        self.embedded_dim += 1  # adding one to embedded dim to take into account token prepend\n",
    "\n",
    "        self.learned_embedding_vec = nn.Parameter(\n",
    "            torch.zeros(self.batch_size, 1, self.position))\n",
    "        \n",
    "        # self.learned_embedding_vec = nn.Parameter(\n",
    "        #     torch.zeros(self.batch_size, 1, self.position))\n",
    "\n",
    "        self.positional_matrix = torch.zeros(\n",
    "            self.embedded_dim, self.position)\n",
    "        \n",
    "        # self.positional_matrix = torch.zeros(\n",
    "        #     self.embedded_dim, self.position)\n",
    "\n",
    "        for pos in range(self.position):\n",
    "            for i in range(int(self.embedded_dim/2)):\n",
    "                denom = pow(n, 2*i/self.embedded_dim)\n",
    "                self.positional_matrix[2*i, pos] = np.sin(pos/denom)\n",
    "                self.positional_matrix[2*i+1, pos] = np.cos(pos/denom)\n",
    "\n",
    "        self.positional_matrix = self.positional_matrix[None, :, :]\n",
    "        self.positional_matrix = self.positional_matrix.tile(\n",
    "            (self.batch_size, 1, 1))\n",
    "\n",
    "    def forward(self, data):\n",
    "        # print(f'self.learned_embedding_vec LOCATION: {self.learned_embedding_vec.get_device()}')\n",
    "        # print(f'data LOCATION: {data.get_device()}')\n",
    "        # print('-----------------------------------------------')\n",
    "        \n",
    "        data = torch.hstack((self.learned_embedding_vec, data))\n",
    "        summer_matrix = data + self.positional_matrix\n",
    "        summer_matrix = self.dropout(summer_matrix)\n",
    "\n",
    "        return summer_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "bca5ba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, num_patch: int = 49):\n",
    "        super(ConvLayer, self).__init__()\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            device = 'cuda'\n",
    "\n",
    "        self.num_patch = num_patch\n",
    "        # self.batch_size = batch_size\n",
    "        n = num_patch\n",
    "#         self.conv2d_1 = nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size = 13, stride = 1)\n",
    "        self.conv2d_1 = nn.Conv2d(\n",
    "            in_channels=n*1, out_channels=n * 8, kernel_size=13, stride=1, groups=n)\n",
    "\n",
    "        self.pooling2d_1 = nn.MaxPool2d(2)\n",
    "\n",
    "        self.conv2d_2 = nn.Conv2d(\n",
    "            in_channels=n*8, out_channels=n*16, kernel_size=11, stride=1, groups=n)\n",
    "\n",
    "        self.pooling2d_2 = nn.MaxPool2d(2)\n",
    "\n",
    "#         self.conv2d_3 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 9, stride = 1, groups = n)\n",
    "        self.conv2d_3 = nn.Conv2d(\n",
    "            in_channels=n*16, out_channels=n*32, kernel_size=9, stride=1, groups=n)\n",
    "\n",
    "\n",
    "#         self.conv2d_4 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 7, stride = 1, groups = n)\n",
    "        self.conv2d_4 = nn.Conv2d(\n",
    "            in_channels=n*32, out_channels=n*32, kernel_size=7, stride=1, groups=n)\n",
    "\n",
    "        self.pooling2d_3 = nn.MaxPool2d(2)\n",
    "\n",
    "#         self.conv2d_5 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 5, stride = 1, groups = n)\n",
    "        self.conv2d_5 = nn.Conv2d(\n",
    "            in_channels=n*32, out_channels=n*64, kernel_size=5, stride=1, groups=n)\n",
    "\n",
    "        self.dnn = nn.Linear(105280, 256)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "    def forward(self, tensor, batch):\n",
    "\n",
    "        # print('IN FORWARD OF CONV LAYER')\n",
    "        # batch_size = tensor.shape()[0]\n",
    "        tensor = tensor[:,]\n",
    "        # print(f'THIS IS THE SHAPE OF THE TENSOR: {tensor.shape}')\n",
    "        x = self.conv2d_1(tensor)\n",
    "        x = self.relu(x)\n",
    "        x = self.pooling2d_1(x)\n",
    "\n",
    "        x = self.conv2d_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pooling2d_2(x)\n",
    "\n",
    "        x = self.conv2d_3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2d_4(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.pooling2d_3(x)\n",
    "\n",
    "        x = self.conv2d_5(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        # print(x.shape)\n",
    "        x = torch.reshape(x, (batch, self.num_patch, 105280))\n",
    "        x = self.dnn(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "a1502f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer_reshaped(nn.Module):\n",
    "    def __init__(self, num_patch: int = 49 ):\n",
    "        super(ConvLayer_reshaped, self).__init__()\n",
    "        self.num_patch = num_patch\n",
    "#         self.batch_size = batch_size\n",
    "        n = num_patch\n",
    "#         self.conv2d_1 = nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size = 13, stride = 1)\n",
    "        self.conv2d_1 = nn.Conv2d(in_channels = n*1, out_channels = n *8, kernel_size = 11, stride = 1, groups = n)\n",
    "        \n",
    "#         self.pooling2d_1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv2d_2 = nn.Conv2d(in_channels = n*8, out_channels = n*16, kernel_size = 9, stride = 1, groups = n)\n",
    "\n",
    "#         self.pooling2d_2 = nn.MaxPool2d(2)\n",
    "        \n",
    "#         self.conv2d_3 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 9, stride = 1, groups = n)\n",
    "        self.conv2d_3 = nn.Conv2d(in_channels = n*16, out_channels = n*32, kernel_size = 7, stride = 1, groups = n)\n",
    "\n",
    "        \n",
    "#         self.conv2d_4 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 7, stride = 1, groups = n)\n",
    "        self.conv2d_4 = nn.Conv2d(in_channels = n*32, out_channels = n*32, kernel_size = 5, stride = 1, groups = n)\n",
    "\n",
    "#         self.pooling2d_3 = nn.MaxPool2d(2)\n",
    "        \n",
    "#         self.conv2d_5 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 5, stride = 1, groups = n)\n",
    "        self.conv2d_5 = nn.Conv2d(in_channels = n*32, out_channels = n*64, kernel_size = 3, stride = 1, groups = n)\n",
    "\n",
    "\n",
    "        \n",
    "        self.dnn = nn.Linear(256,128)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "\n",
    "    def forward(self, tensor,batch):\n",
    "        # print('IN FORWARD OF CONV LAYER')\n",
    "        tensor = tensor[:,]\n",
    "        print(tensor.shape)\n",
    "        # print(f'THIS IS THE SHAPE OF THE TENSOR: {tensor.shape}')\n",
    "        x = self.conv2d_1(tensor)\n",
    "        x = self.relu(x)\n",
    "#         x = self.pooling2d_1(x)\n",
    "        print(x.shape)\n",
    "\n",
    "        x = self.conv2d_2(x)\n",
    "        x = self.relu(x)\n",
    "#         x = self.pooling2d_2(x)\n",
    "\n",
    "        x = self.conv2d_3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2d_4(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "#         x = self.pooling2d_3(x)\n",
    "\n",
    "        x = self.conv2d_5(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        print(x.shape)\n",
    "        val = x.shape[1]/self.num_patch\n",
    "        x = torch.reshape(x, (batch, self.num_patch, int(val)))\n",
    "        print(val)\n",
    "        x = self.dnn(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "16d8ad7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingBlock(nn.Module):\n",
    "    # Data in this sense is the image that has not been translated into an array\n",
    "    # Want to set x_con to 3500\n",
    "    def __init__(self, x_amount=7, y_amount=7, x_con=3500, y_con=2800):\n",
    "        super(EmbeddingBlock, self).__init__()\n",
    "\n",
    "        assert (x_con % x_amount == 0)\n",
    "#         print(y_con)\n",
    "#         print(y_amount)\n",
    "        assert (y_con % y_amount == 0)\n",
    "        self.x_amount = x_amount\n",
    "        self.y_amount = y_amount\n",
    "        self.x_con = x_con\n",
    "        self.y_con = y_con\n",
    "\n",
    "        self.amount_of_patches = int(x_amount * y_amount)\n",
    "        self.x_ran = int(x_con / x_amount)\n",
    "        self.y_ran = int(y_con / y_amount)\n",
    "        self.patches_matrix = torch.zeros(\n",
    "            self.amount_of_patches, self.x_ran, self.y_ran)\n",
    "\n",
    "        self.cc_conv = ConvLayer_reshaped(num_patch = self.amount_of_patches)\n",
    "        self.mlo_conv = ConvLayer_reshaped(num_patch = self.amount_of_patches)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # recheck for proper class variables(change self. to strictly local variable)\n",
    "        # print('IN FORWARD OF EMBEDDING BLOCK LAYER')\n",
    "        # Data shape (batch size, num of views, x_length, y_length)\n",
    "        info = data\n",
    "\n",
    "        batch_size = info.shape[0]\n",
    "\n",
    "        batched_patches = info.unfold(\n",
    "            2, self.x_ran, self.x_ran).unfold(3, self.y_ran, self.y_ran)\n",
    "        batched_patches = torch.reshape(batched_patches,\n",
    "                                        (batch_size, 4, self.amount_of_patches, self.x_ran, self.y_ran))\n",
    "\n",
    "        # Reshape now makes data (batch_size,4,49,500,400)\n",
    "\n",
    "        # print(f'THIS IS DATA LOCATION IN FORWARD OF EMBEDDING BLOCK: {data.get_device()}')\n",
    "\n",
    "\n",
    "        LCC = batched_patches[:, 0]\n",
    "        LMLO = batched_patches[:, 1]\n",
    "        RCC = batched_patches[:, 2]\n",
    "        RMLO = batched_patches[:, 3]\n",
    "\n",
    "        LCC = self.cc_conv.forward(LCC, batch_size )\n",
    "        RCC = self.cc_conv.forward(RCC, batch_size )\n",
    "        LMLO = self.mlo_conv.forward(LMLO, batch_size )\n",
    "        RMLO = self.mlo_conv.forward(RMLO, batch_size )\n",
    "\n",
    "        pos_encoding_LCC = PositionalEncoding(LCC)\n",
    "        pos_encoding_RCC = PositionalEncoding(RCC)\n",
    "        pos_encoding_LMLO = PositionalEncoding(LMLO)\n",
    "        pos_encoding_RMLO = PositionalEncoding(RMLO)\n",
    "\n",
    "        summer_LCC = pos_encoding_LCC.forward(LCC)\n",
    "        summer_RCC = pos_encoding_RCC.forward(RCC)\n",
    "        summer_LMLO = pos_encoding_LMLO.forward(LMLO)\n",
    "        summer_RMLO = pos_encoding_RMLO.forward(RMLO)\n",
    "\n",
    "        batched_positional_encoding = torch.zeros(batch_size, 4, summer_LCC.shape[1], summer_LCC.shape[2])\n",
    "\n",
    "        batched_positional_encoding[:, 0] = summer_LCC\n",
    "        batched_positional_encoding[:, 1] = summer_LMLO\n",
    "        batched_positional_encoding[:, 2] = summer_RCC\n",
    "        batched_positional_encoding[:, 3] = summer_RMLO\n",
    "\n",
    "        # print(f'THIS IS BATCHED POSITIONAL ENCODING LOCATION: {batched_positional_encoding.get_device()}')\n",
    "\n",
    "        return batched_positional_encoding, batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "727dab82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_layer,hidden_output=1024, dropout=.5):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fnn1 = nn.Linear(input_layer, hidden_output)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.fnn2 = nn.Linear(hidden_output, input_layer)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # print('IN FORWARD OF MLP LAYER')\n",
    "        x = self.fnn1(data)\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fnn2(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "c325cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalEncoderBlock(nn.Module):\n",
    "    def __init__(self, data_shape, hidden_output_fnn1=1024, dropout=.5):\n",
    "        super(LocalEncoderBlock, self).__init__()\n",
    "        # self.device = 'cpu'\n",
    "        # if torch.cuda.is_available():\n",
    "        #     self.device = 'cuda'\n",
    "\n",
    "        self.data_shape = data_shape\n",
    "        # Layer norm over the H and W of each image\n",
    "        self.batch_size = 10\n",
    "#         print([data_shape[2], data_shape[3]])\n",
    "        # self.ln1 = nn.LayerNorm(\n",
    "        #     [data_shape[2], data_shape[3]], device=self.device)\n",
    "        \n",
    "        self.ln1 = nn.LayerNorm(\n",
    "            [data_shape[2], data_shape[3]])\n",
    "        \n",
    "\n",
    "        # self.ln2 = nn.LayerNorm(\n",
    "        #     [data_shape[2], data_shape[3]], device=self.device)\n",
    "        \n",
    "        self.ln2 = nn.LayerNorm(\n",
    "            [data_shape[2], data_shape[3]])\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=data_shape[3], num_heads=16, batch_first=True)\n",
    "        self.mlp_0 = MLP(input_layer=data_shape[3],\n",
    "            hidden_output=hidden_output_fnn1, dropout=dropout)\n",
    "        self.mlp_1 = MLP(input_layer=data_shape[3],\n",
    "            hidden_output=hidden_output_fnn1, dropout=dropout)\n",
    "        self.mlp_2 = MLP(input_layer=data_shape[3],\n",
    "            hidden_output=hidden_output_fnn1, dropout=dropout)\n",
    "        self.mlp_3 = MLP(input_layer=data_shape[3],\n",
    "            hidden_output=hidden_output_fnn1, dropout=dropout)\n",
    "\n",
    "    def forward(self, data, batch):\n",
    "\n",
    "        data_shape = (batch,self.data_shape[1],self.data_shape[2],self.data_shape[3])\n",
    "        # data.to(self.device)\n",
    "        device = data.get_device()\n",
    "        x_tilda_matrix = torch.zeros(data_shape)\n",
    "\n",
    "        attn_0, y = self.helper_thing(data[:, 0])\n",
    "        x_tilda_matrix[:, 0] = y\n",
    "        attn_1, y = self.helper_thing(data[:, 1])\n",
    "        x_tilda_matrix[:, 1] = y\n",
    "        attn_2, y = self.helper_thing(data[:, 2])\n",
    "        x_tilda_matrix[:, 2] = y\n",
    "        attn_3, y = self.helper_thing(data[:, 3])\n",
    "        x_tilda_matrix[:, 3] = y\n",
    "\n",
    "        dnn_output = torch.zeros(data_shape)\n",
    "        dnn_output[:, 0] = self.mlp_0.forward(attn_0)\n",
    "        dnn_output[:, 1] = self.mlp_1.forward(attn_1)\n",
    "        dnn_output[:, 2] = self.mlp_2.forward(attn_2)\n",
    "        dnn_output[:, 3] = self.mlp_3.forward(attn_3)\n",
    "        x_second = dnn_output + x_tilda_matrix\n",
    "\n",
    "        return x_second\n",
    "\n",
    "    def helper_thing(self, data):\n",
    "        # Data should be of shape (batch_size, 256, 50)\n",
    "\n",
    "        # data = data.to(self.device)\n",
    "\n",
    "        # print(self.device)\n",
    "        # print(f'Data Device:{data.device}')\n",
    "        # print(f'ln1 device: {self.ln1.device}')\n",
    "        x = self.ln1(data)\n",
    "#         print(f'x.shape: {x.shape}')\n",
    "        att_out, att_out_weights = self.attention(\n",
    "            query=x, key=x, value=x)\n",
    "        \n",
    "        # att_out = att_out.to(self.device)\n",
    "\n",
    "        x_tilda = att_out + data\n",
    "        x_second = self.ln2(x_tilda)\n",
    "#         print(x_tilda.shape)\n",
    "\n",
    "        return x_second, x_tilda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "e03fae78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualTransformer(nn.Module):\n",
    "    # embedding parameters, local encoder parameters\n",
    "    def __init__(self, rank, x_amount=7, y_amount=7, x_con=3500, y_con=2800,\n",
    "                 data_shape=(10, 4, 50, 256), hidden_output_fnn=1024, dropout=.5,\n",
    "                 number_of_layers=10):\n",
    "        super(VisualTransformer, self).__init__()\n",
    "        self.rank = rank\n",
    "        self.embedding_block= EmbeddingBlock(\n",
    "                                              x_amount=x_amount, y_amount=y_amount, x_con=x_con, y_con=y_con)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(number_of_layers):\n",
    "            self.blks.add_module(\n",
    "                f'{i}', LocalEncoderBlock(data_shape=data_shape))\n",
    "\n",
    "    def forward(self, data):\n",
    "        # print('IN FORWARD OF VISUALTRANSFORMER LAYER')\n",
    "        x, batch = self.embedding_block.forward(data)\n",
    "#         x.to(self.rank)\n",
    "        # print(f'THIS IS THE LOCATION OF X: {x.get_device()}')\n",
    "        # print(x.shape)\n",
    "        i = 0\n",
    "        for blk in self.blks:\n",
    "            # print(f'This is {i} local attention run')\n",
    "            i += 1\n",
    "            x = blk(x, batch)\n",
    "\n",
    "\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "925121b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalEncoderBlock(nn.Module):\n",
    "    def __init__(self, data_shape=(200, 256), hidden_output_fnn1=1024, dropout=.5):\n",
    "        super(GlobalEncoderBlock, self).__init__()\n",
    "        self.device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = 'cuda'\n",
    "\n",
    "        self.data_shape = data_shape\n",
    "        # self.gln1 = nn.LayerNorm(data_shape, device=self.device)\n",
    "\n",
    "        self.gln1 = nn.LayerNorm(data_shape)\n",
    "\n",
    "        # self.ln2 = nn.LayerNorm(data_shape, device=self.device)\n",
    "\n",
    "        self.ln2 = nn.LayerNorm(data_shape)\n",
    "\n",
    "        self.attention = nn.MultiheadAttention(\n",
    "            embed_dim=data_shape[1], num_heads=16, batch_first=True)\n",
    "        self.mlp = MLP(input_layer = data_shape[1], hidden_output=hidden_output_fnn1, dropout=dropout)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # print('IN FORWARD OF GLOBALENCODERBLOCK LAYER')\n",
    "        # data = data.to(self.device)\n",
    "\n",
    "        x = self.gln1(data)\n",
    "        att_out, att_out_weights = self.attention(query=x, key=x, value=x)\n",
    "       \n",
    "        # att_out = att_out.to(self.device)\n",
    "\n",
    "        x_tilda = att_out + data\n",
    "        x_second = self.ln2(x_tilda)\n",
    "        dnn_output = self.mlp.forward(x_second)\n",
    "        x_second = dnn_output + x_tilda\n",
    "\n",
    "        return x_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "483b7344",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalTransformer(nn.Module):\n",
    "    def __init__(self, x_amount=7, y_amount=7, x_con=3500, y_con=2800,\n",
    "                 data_shape=(10, 4, 50, 256), hidden_output_fnn=1024, dropout=.5,\n",
    "                 number_of_layers=10, num_layers_global=10):\n",
    "        super(GlobalTransformer, self).__init__()\n",
    "        self.data_shape = data_shape\n",
    "        new_data_shape = (data_shape[1]\n",
    "                          * data_shape[2], data_shape[3])\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers_global):\n",
    "            self.blks.add_module(\n",
    "                f'{i}', GlobalEncoderBlock(data_shape=new_data_shape))\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # self.class_head = classification_head(input_layer=data_shape[0]*data_shape[2],\n",
    "        #                                       hidden_output_class=512, dropout=.5)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # print('IN FORWARD OF GLOBALTRANSFORMER LAYER')\n",
    "        #x = self.individual_transformer.forward(data)\n",
    "\n",
    "        shape0, shape1, shape2, shape3 = data.shape\n",
    "        x = torch.reshape(data, (shape0, shape1 * shape2, shape3))\n",
    "        i = 0\n",
    "        for blk in self.blks:\n",
    "            # print(f'This is {i} global attention run')\n",
    "            x = blk(x)\n",
    "            i += 1\n",
    "\n",
    "#         x = torch.squeeze(x)\n",
    "        # print(x.shape)\n",
    "        x = x[:, [0, 1 * shape2, 2 * shape2, 3 * shape2], :]\n",
    "        x = torch.reshape(x, (x.shape[0], x.shape[1]*x.shape[2]))\n",
    "#         print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "dd959119",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, input_layer=1024, hidden_output_class=512, dropout=0.5):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        self.ln1 = nn.LayerNorm(input_layer)\n",
    "        self.fnn1 = nn.Linear(input_layer, hidden_output_class)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln2 = nn.LayerNorm(hidden_output_class)\n",
    "        self.fnn2 = nn.Linear(hidden_output_class, 5)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # print('IN FORWARD OF CLASSIFICATIONHEAD LAYER')\n",
    "        x = self.ln1(data)\n",
    "        x = self.fnn1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.ln2(x)\n",
    "        x = self.fnn2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "4d7353f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionHead(nn.Module):\n",
    "    def __init__(self, input_layer=1024, hidden_output_class=512, dropout=0.5):\n",
    "        super(RegressionHead, self).__init__()\n",
    "        self.ln1 = nn.LayerNorm(input_layer)\n",
    "        self.fnn1 = nn.Linear(input_layer, hidden_output_class)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln2 = nn.LayerNorm(hidden_output_class)\n",
    "        self.fnn2 = nn.Linear(hidden_output_class, 1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # print('IN FORWARD OF REGRESSIONHEAD LAYER')\n",
    "        x = self.ln1(data)\n",
    "        x = self.fnn1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.ln2(x)\n",
    "        x = self.fnn2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "0befc741",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaperModel(nn.Module):\n",
    "    def __init__(self, x_amount=7, y_amount=7, x_con=3500, y_con=2800,\n",
    "                 data_shape=(10, 4, 50, 256), hidden_output_fnn=1024, dropout=.5,\n",
    "                 number_of_layers=10, num_layers_global=10, setting='C'):\n",
    "\n",
    "        assert setting in {'C', 'R'}\n",
    "\n",
    "        super(PaperModel, self).__init__()\n",
    "\n",
    "        # print(f\"THIS IS THE SUPPOSED RANK: {rank}\")\n",
    "\n",
    "        self.rank = 0\n",
    "        rank = 0\n",
    "\n",
    "        # self.embedding_block = EmbeddingBlock(batch=data_shape[0],\n",
    "        #                                       x_amount=x_amount, y_amount=y_amount, x_con=x_con, y_con=y_con)\n",
    "\n",
    "        self.visual_transformer = VisualTransformer(rank, x_amount, y_amount, x_con, y_con,\n",
    "                                                    data_shape, hidden_output_fnn, dropout,\n",
    "                                                    number_of_layers)\n",
    "\n",
    "        self.global_transformer = GlobalTransformer(x_amount, y_amount, x_con, y_con,\n",
    "                                                    data_shape, hidden_output_fnn, dropout,\n",
    "                                                    number_of_layers, num_layers_global)\n",
    "\n",
    "        if setting == 'C':\n",
    "\n",
    "            self.left_head = ClassificationHead(\n",
    "                input_layer=data_shape[3]*4, hidden_output_class=512, dropout=0.5)\n",
    "\n",
    "            self.right_head = ClassificationHead(\n",
    "                input_layer=data_shape[3]*4, hidden_output_class=512, dropout=0.5)\n",
    "\n",
    "        elif setting == 'R':\n",
    "            self.left_head = RegressionHead(\n",
    "                input_layer=data_shape[3]*4, hidden_output_class=512, dropout=0.5)\n",
    "\n",
    "            self.right_head = RegressionHead(\n",
    "                input_layer=data_shape[3]*4, hidden_output_class=512, dropout=0.5)\n",
    "\n",
    "    def forward(self, data):\n",
    "        #X = self.embedding_block(data)\n",
    "        batch = data.shape[0]\n",
    "\n",
    "        #data = torch.reshape(data, (4,3500,2800))\n",
    "        # here\n",
    "        # print(f'THIS IS THE DATA SHAPE: {data.shape}')\n",
    "\n",
    "\n",
    "\n",
    "        X = self.visual_transformer(data)\n",
    "\n",
    "        X = self.global_transformer(X)\n",
    "\n",
    "        left_pred = self.left_head(X)\n",
    "\n",
    "        # print(f'left shape: {left_pred.shape}')\n",
    "\n",
    "        right_pred = self.right_head(X)\n",
    "        # print(f'right shape: {right_pred.shape}')\n",
    "\n",
    "        # final = torch.zeros(left_pred.shape[0], 5, 2)\n",
    "        # final[:, :, 0] = left_pred\n",
    "        # final[:, :, 1] = right_pred\n",
    "\n",
    "        # CE LOSS\n",
    "        final = torch.stack((left_pred, right_pred), dim=2)\n",
    "\n",
    "        #Regression Loss\n",
    "        # final = torch.cat((left_pred, right_pred), dim=1)\n",
    "\n",
    "        # print(f'left_pred: {left_pred}')\n",
    "        # print(f'left_pred.shape: {left_pred.shape}')\n",
    "\n",
    "        # print(f'right_pred: {right_pred}')\n",
    "        # print(f'right_pred.shape: {right_pred.shape}')\n",
    "\n",
    "        \n",
    "\n",
    "        # print(f'Finished data classification, returning vectors of shape: {final.shape}')\n",
    "\n",
    "        return final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "4d962c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(4,4,448,448)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "f5f041a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pap = PaperModel(x_amount=14, y_amount=14,x_con=448, y_con=448, data_shape = (1,4,197,128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "9e0fb4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 196, 32, 32])\n",
      "torch.Size([4, 1568, 22, 22])\n",
      "torch.Size([4, 50176])\n",
      "256.0\n",
      "torch.Size([4, 196, 32, 32])\n",
      "torch.Size([4, 1568, 22, 22])\n",
      "torch.Size([4, 50176])\n",
      "256.0\n",
      "torch.Size([4, 196, 32, 32])\n",
      "torch.Size([4, 1568, 22, 22])\n",
      "torch.Size([4, 50176])\n",
      "256.0\n",
      "torch.Size([4, 196, 32, 32])\n",
      "torch.Size([4, 1568, 22, 22])\n",
      "torch.Size([4, 50176])\n",
      "256.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2024, -0.1152],\n",
       "         [-0.6624,  0.3832],\n",
       "         [ 1.2930, -0.4327],\n",
       "         [-0.5887, -0.5120],\n",
       "         [-0.6203, -0.2593]],\n",
       "\n",
       "        [[-0.3676,  0.2887],\n",
       "         [-0.1708,  0.2555],\n",
       "         [ 0.8693, -0.0249],\n",
       "         [-0.6097, -0.1596],\n",
       "         [-0.4263, -0.1083]],\n",
       "\n",
       "        [[ 0.6837,  0.0692],\n",
       "         [ 0.4072,  0.8835],\n",
       "         [-0.2167, -0.2985],\n",
       "         [ 0.4898,  0.2625],\n",
       "         [ 0.1071,  0.1328]],\n",
       "\n",
       "        [[-0.5538,  0.2826],\n",
       "         [ 0.0886,  0.2762],\n",
       "         [ 0.3469, -0.0806],\n",
       "         [-0.6703,  0.0067],\n",
       "         [-0.6246, -0.0097]]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pap.forward(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
