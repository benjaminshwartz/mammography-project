{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e90fbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input arrays are (3500, 2800)\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import pydicom as dicom\n",
    "import math\n",
    "import time\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "23012de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0\n"
     ]
    }
   ],
   "source": [
    "print(torch. __version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f4bd899",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Positional encoding class\n",
    "#May want to scrap this for a learnable positional encoding model as opposed to sinusoidal \n",
    "class Positional_Encoding(nn.Module):\n",
    "    def __init__(self,data,dropout=0.1,n = 10000):\n",
    "        super(Positional_Encoding, self).__init__()\n",
    "        self.embedded_dim, self.position = data.shape\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.embedded_dim += 1 #adding one to embedded dim to take into account token prepend\n",
    "        \n",
    "        self.learned_embedding_vec = nn.Parameter(torch.zeros(1,self.position))\n",
    "        \n",
    "        self.positional_matrix = torch.zeros(self.embedded_dim,self.position)\n",
    "\n",
    "        for pos in range(self.position):\n",
    "            for i in range(int(self.embedded_dim/2)):\n",
    "                denom = pow(n, 2*i/self.embedded_dim) \n",
    "                self.positional_matrix[2*i,pos] = np.sin(pos/denom)\n",
    "                self.positional_matrix[2*i+1,pos] = np.cos(pos/denom)     \n",
    "    \n",
    "    def forward(self,data):\n",
    "#         print(f'Data shape: {data.shape}')\n",
    "#         print(f'positional_matrix shape: {self.positional_matrix.shape}')\n",
    "        self.data = torch.vstack((self.learned_embedding_vec,data))\n",
    "        self.summer_matrix = self.data + self.positional_matrix\n",
    "        self.summer_matrix = self.dropout(self.summer_matrix)\n",
    "\n",
    "        return self.summer_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "31ee2fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply conv layer to a (500, 400) subset of each scan \n",
    "#TODO max pool is necessary \n",
    "class convlayer(nn.Module):\n",
    "    def __init__(self, num_patch: int = 49):\n",
    "        super(convlayer, self).__init__()\n",
    "        self.num_patch = num_patch\n",
    "        n = num_patch\n",
    "#         self.conv2d_1 = nn.Conv2d(in_channels = 1, out_channels = 8, kernel_size = 13, stride = 1)\n",
    "        self.conv2d_1 = nn.Conv2d(in_channels = n*1, out_channels = n *8, kernel_size = 13, stride = 1, groups = n)\n",
    "        \n",
    "        self.pooling2d_1 = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.conv2d_2 = nn.Conv2d(in_channels = n*8, out_channels = n*16, kernel_size = 11, stride = 1, groups = n)\n",
    "\n",
    "        self.pooling2d_2 = nn.MaxPool2d(2)\n",
    "        \n",
    "#         self.conv2d_3 = nn.Conv2d(in_channels = 16, out_channels = 32, kernel_size = 9, stride = 1, groups = n)\n",
    "        self.conv2d_3 = nn.Conv2d(in_channels = n*16, out_channels = n*32, kernel_size = 9, stride = 1, groups = n)\n",
    "\n",
    "        \n",
    "#         self.conv2d_4 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 7, stride = 1, groups = n)\n",
    "        self.conv2d_4 = nn.Conv2d(in_channels = n*32, out_channels = n*32, kernel_size = 7, stride = 1, groups = n)\n",
    "\n",
    "        self.pooling2d_3 = nn.MaxPool2d(2)\n",
    "        \n",
    "#         self.conv2d_5 = nn.Conv2d(in_channels = 32, out_channels = 64, kernel_size = 5, stride = 1, groups = n)\n",
    "        self.conv2d_5 = nn.Conv2d(in_channels = n*32, out_channels = n*64, kernel_size = 5, stride = 1, groups = n)\n",
    "\n",
    "\n",
    "        \n",
    "        self.dnn = nn.Linear(105280,256)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        \n",
    "    def forward(self, tensor):\n",
    "        print(f'SHAPE OF THE TENSOR: {tensor.shape}')\n",
    "        x = self.conv2d_1(tensor)\n",
    "        x = self.relu(x)\n",
    "        x = self.pooling2d_1(x)\n",
    "        \n",
    "        x = self.conv2d_2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pooling2d_2(x)\n",
    "        \n",
    "        x = self.conv2d_3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2d_4(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.pooling2d_3(x)\n",
    "        \n",
    "        x = self.conv2d_5(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = torch.reshape(x,(self.num_patch,105280))\n",
    "        #print(f'x.shape in forward in conv layer {x.shape}')\n",
    "        x = self.dnn(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77dfa275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 500, 400])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=49, weight of size [392, 1, 13, 13], expected input[1, 1, 500, 400] to have 49 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(sample\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m convlayer()\n\u001b[0;32m----> 4\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mconvlayer.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor):\n\u001b[0;32m---> 38\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     40\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling2d_1(x)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=49, weight of size [392, 1, 13, 13], expected input[1, 1, 500, 400] to have 49 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "sample = torch.zeros(1,500,400)\n",
    "print(sample.shape)\n",
    "model = convlayer()\n",
    "x = model.forward(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1644d7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=49, weight of size [392, 1, 13, 13], expected input[1, 1, 500, 400] to have 49 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     patch \u001b[38;5;241m=\u001b[39m patch[\u001b[38;5;28;01mNone\u001b[39;00m,:,:]\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m#print(patch.shape)\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#     embedded_patches.append(x.T)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#     print(x)\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     embedded_matrix[:,i] \u001b[38;5;241m=\u001b[39m x\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mconvlayer.forward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor):\n\u001b[0;32m---> 38\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[1;32m     40\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooling2d_1(x)\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/miniconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=49, weight of size [392, 1, 13, 13], expected input[1, 1, 500, 400] to have 49 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "#Test with one image \n",
    "ds = dicom.dcmread('2ddfad7286c2b016931ceccd1e2c7bbc copy.dicom')\n",
    "info = ds.pixel_array[:3500,:]\n",
    "list_of_patches = []\n",
    "patches_matrix = torch.zeros(49,500,400)\n",
    "for i in range(7):\n",
    "    x_axis = [i*500,(i+1)*500]\n",
    "    for j in range(7):\n",
    "        y_axis = [j*400,(j+1)*400]\n",
    "        correct_patch = np.ascontiguousarray(info[x_axis[0]:x_axis[1],y_axis[0]:y_axis[1]],dtype = np.float32)\n",
    "        tensor = torch.from_numpy(correct_patch)\n",
    "#         list_of_patches.append(tensor)\n",
    "        loc = 7*i + j\n",
    "        patches_matrix[loc,:,:] = tensor\n",
    "# print(len(list_of_patches))\n",
    "# print(patches_matrix.shape[0])\n",
    "# print(list_of_patches[1])\n",
    "# print(patches_matrix[1,:,:])\n",
    "embedded_patches = []\n",
    "embedded_matrix = torch.zeros(256,49)\n",
    "for i in range(patches_matrix.shape[0]):\n",
    "    patch = patches_matrix[i,:,:]\n",
    "#     print(patch.shape)\n",
    "    model = convlayer()\n",
    "    patch = patch[None,:,:]\n",
    "    #print(patch.shape)\n",
    "    x = model.forward(patch)\n",
    "#     embedded_patches.append(x.T)\n",
    "#     print(x)\n",
    "    embedded_matrix[:,i] = x\n",
    "# Learnable embedding\n",
    "learned_embedding_vec = nn.Parameter(torch.zeros(256,1))\n",
    "#nn.Parameter adds it to model paramter so that we can backprop through it \n",
    "embedded_matrix = torch.hstack((learned_embedding_vec,embedded_matrix))\n",
    "\n",
    "#     print(embedded_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc103605",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m positional \u001b[38;5;241m=\u001b[39m Positional_Encoding(embedded_matrix)\n\u001b[0;32m----> 2\u001b[0m summer, pos \u001b[38;5;241m=\u001b[39m positional\u001b[38;5;241m.\u001b[39mforward(embedded_matrix)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "positional = Positional_Encoding(embedded_matrix)\n",
    "summer, pos = positional.forward(embedded_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41176914",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full Embedding class that takes in data name for individual image and outputs positional embedding where each column\n",
    "#vector represents a positionally-embedded patch except for the very first column vector, which is a learnt \n",
    "#classification token \n",
    "#TODO finish reimplement for  batched input\n",
    "\n",
    "class embedding_block(nn.Module):\n",
    "    #Data in this sense is the image that has not been translated into an array\n",
    "    #Want to set x_con to 3500\n",
    "    def __init__(self, x_amount = 7, y_amount = 7, x_con = 3500, y_con = 2800):\n",
    "        super(embedding_block, self).__init__()\n",
    "        \n",
    "        assert(x_con % x_amount == 0)\n",
    "        assert(y_con % y_amount == 0)\n",
    "        self.x_amount = x_amount\n",
    "        self.y_amount = y_amount\n",
    "        self.x_con = x_con\n",
    "        self.y_con = y_con\n",
    "        \n",
    "        \n",
    "        self.amount_of_patches = int(x_amount * y_amount)\n",
    "        self.x_ran = int(x_con / x_amount)\n",
    "        self.y_ran = int(y_con / y_amount)\n",
    "        self.patches_matrix = torch.zeros(self.amount_of_patches,self.x_ran,self.y_ran)\n",
    "        #print(self.patches_matrix.shape)\n",
    "        \n",
    "        #print('here')\n",
    "        \n",
    "        # TODO : add conv1 and conv2\n",
    "        \n",
    "    \n",
    "    def forward(self, data):\n",
    "        # recheck for proper class variables(change self. to strictly local variable)\n",
    "        self.info = data[:,:self.x_con,:self.y_con]\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.batch_size = self.info.shape[0]\n",
    "        \n",
    "        self.batched_patches = self.info.unfold(1,self.x_ran,self.x_ran).unfold(2,self.y_ran,self.y_ran)\n",
    "        self.batched_patches = torch.reshape(self.batched_patches,\n",
    "                                             (self.batch_size,self.amount_of_patches,self.x_ran,self.y_ran))\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        self.LCC = self.batched_patches[0]\n",
    "        print(f'LCC SHAPE:{self.LCC.shape}')\n",
    "        #print(f'self.LCC shape before conv: {self.LCC.shape}')\n",
    "        self.LMLO = self.batched_patches[1]\n",
    "        self.RCC = self.batched_patches[2]\n",
    "        self.RMLO = self.batched_patches[3]\n",
    "        \n",
    "#         Dealing with (RCC, LCC)  and (RMLO, LMLO)\n",
    "#         init 2 conv layers that: \n",
    "#              - torch.Size([B, 49, 500, 400]) apply some function that applies 49 independent conv \n",
    "#              layers to dim 1 i.e. clever use of reshape/channels/batch_size/groups\n",
    "#              - torch.Size([B, 49, 256])\n",
    "        \n",
    "#         init -> conv_1\n",
    "        self.cc_conv = convlayer()\n",
    "        self.mlo_conv = convlayer()\n",
    "        \n",
    "        self.LCC = self.cc_conv.forward(self.LCC)\n",
    "        #print(f'self.LCC shape after conv: {self.LCC.shape}')\n",
    "        self.RCC = self.cc_conv.forward(self.RCC)\n",
    "        \n",
    "        self.LMLO = self.mlo_conv.forward(self.LMLO)\n",
    "        self.RMLO = self.mlo_conv.forward(self.RMLO)\n",
    "#         init -> conv_2 \n",
    "        \n",
    "#         rcc_after_conv = conv_1(RCC)\n",
    "#         lcc_after_conv = conv_1(mirror(LCC))\n",
    "#              # or do RCC cat mirror(LCC) ==> torch.Size([B*2, 49, 256])\n",
    "        \n",
    "        \n",
    "#         conv_2(RMLO)\n",
    "#         conv_2(mirror(LMLO))\n",
    "\n",
    "            \n",
    "#         self.learned_embedding_vec = nn.Parameter(torch.zeros(256,1))\n",
    "        \n",
    "        #nn.Parameter adds it to model paramter so that we can backprop through it \n",
    "#         self.embedded_matrix = torch.v=hstack((self.learned_embedding_vec,self.embedded_matrix))\n",
    "         \n",
    "#         print(f'self.embedded_matrix shape: {self.embedded_matrix.shape}')\n",
    "        #TODO MOVE TO INIT\n",
    "        self.pos_encoding_LCC = Positional_Encoding(self.LCC)\n",
    "        self.pos_encoding_RCC = Positional_Encoding(self.RCC)\n",
    "        self.pos_encoding_LMLO = Positional_Encoding(self.LMLO)\n",
    "        self.pos_encoding_RMLO = Positional_Encoding(self.RMLO)\n",
    "        \n",
    "        self.summer_LCC = self.pos_encoding_LCC.forward(self.LCC)\n",
    "        self.summer_RCC = self.pos_encoding_RCC.forward(self.RCC)\n",
    "        self.summer_LMLO = self.pos_encoding_LMLO.forward(self.LMLO)\n",
    "        self.summer_RMLO = self.pos_encoding_RMLO.forward(self.RMLO)\n",
    "        \n",
    "        self.batched_positional_encoding = torch.zeros(self.batch_size,50,256)\n",
    "        #print(self.batched_positional_encoding.shape)\n",
    "        #print(self.summer_LCC.shape)\n",
    "        self.batched_positional_encoding[0] = self.summer_LCC\n",
    "        self.batched_positional_encoding[1] = self.summer_LMLO\n",
    "        self.batched_positional_encoding[2] = self.summer_RCC\n",
    "        self.batched_positional_encoding[3] = self.summer_RMLO\n",
    "#         print(f'positional_matrix shpae: {self.positional.positional_matrix.shape}')\n",
    "#         self.summer, self.pos = self.positional.forward(self.embedded_matrix)\n",
    "        \n",
    "#         print(f'self.summer shape: {self.summer.shape}')\n",
    "        \n",
    "        return self.batched_positional_encoding\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e57a9350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3500, 2800])\n",
      "torch.Size([4, 7, 7, 500, 400])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 49, 500, 400])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = torch.rand(4,3500,2800)\n",
    "print(tensor.shape)\n",
    "# tensor2 = tensor.unfold(0,1,1).unfold(1,500,500).unfold(2,400,400)\n",
    "tensor2 = tensor.unfold(1,500,500).unfold(2,400,400)\n",
    "print(tensor2.shape)\n",
    "tensor3 = torch.reshape(tensor2, (4,49,500,400))\n",
    "tensor3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90dcaba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 50, 256])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = embedding_block(x_con = 3500)\n",
    "y = x.forward(tensor)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f34c70c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3500, 2800])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b0cad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global and Local mlp are equivalent \n",
    "class local_mlp(nn.Module):\n",
    "    def __init__(self, hidden_output = 1024, dropout = .5):\n",
    "        super(local_mlp, self).__init__()\n",
    "        self.fnn1 = nn.Linear(256,hidden_output)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.fnn2 = nn.Linear(hidden_output,256)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "    def forward(self, data):\n",
    "        self.x = self.fnn1(data)\n",
    "        self.x = self.gelu(self.x)\n",
    "        self.x = self.dropout1(self.x)\n",
    "        self.x = self.fnn2(self.x)\n",
    "        self.x = self.gelu(self.x)\n",
    "        self.x = self.dropout2(self.x)\n",
    "        \n",
    "        return self.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bd275e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class local_encoder_block(nn.Module):\n",
    "    def __init__(self, data_shape = (4,50,256), hidden_output_fnn1 = 1024, dropout = .5):\n",
    "        super(local_encoder_block, self).__init__()\n",
    "        self.data_shape = data_shape\n",
    "        #print(data_shape)\n",
    "        self.ln1 = nn.LayerNorm([data_shape[1],data_shape[2]]) #Layer norm over the H and W of each image\n",
    "        self.ln2 = nn.LayerNorm([data_shape[1],data_shape[2]])\n",
    "        \n",
    "        self.attention = nn.MultiheadAttention(embed_dim = 256,num_heads = 16, batch_first = True)\n",
    "        self.mlp_0 = local_mlp(hidden_output = hidden_output_fnn1, dropout = dropout)\n",
    "        self.mlp_1 = local_mlp(hidden_output = hidden_output_fnn1, dropout = dropout)\n",
    "        self.mlp_2 = local_mlp(hidden_output = hidden_output_fnn1, dropout = dropout)\n",
    "        self.mlp_3 = local_mlp(hidden_output = hidden_output_fnn1, dropout = dropout)\n",
    "        \n",
    "        self.dnn_output = torch.zeros(data_shape)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        #i = 0\n",
    "        #print(i)\n",
    "        if data.shape == (4,256,50):\n",
    "            print('in here')\n",
    "            data = data.T\n",
    "        self.x = self.ln1(data)\n",
    "        self.att_out, self.att_out_weights = self.attention(query = self.x,key = self.x,value = self.x)\n",
    "        #print(self.att_out.shape)\n",
    "        self.x_tilda = self.att_out + data\n",
    "        #print(self.x_tilda.shape)\n",
    "        self.x_second = self.ln2(self.x_tilda)\n",
    "        #self.x_second = self.mlp.forward(self.x_second)\n",
    "        self.dnn_output[0] = self.mlp_0.forward(self.x_second[0])\n",
    "        self.dnn_output[1] = self.mlp_1.forward(self.x_second[1])\n",
    "        self.dnn_output[2] = self.mlp_2.forward(self.x_second[2])\n",
    "        self.dnn_output[3] = self.mlp_3.forward(self.x_second[3])\n",
    "        #print(self.x_second.shape)\n",
    "        self.x_second = self.dnn_output + self.x_tilda\n",
    "        \n",
    "        #i += 1\n",
    "        return self.x_second  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44445c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class global_encoder_block(nn.Module):\n",
    "    def __init__(self, data_shape = (1,200,256), hidden_output_fnn1 = 1024, dropout = .5):\n",
    "        super(global_encoder_block, self).__init__()\n",
    "        self.data_shape = data_shape\n",
    "        self.gln1 = nn.LayerNorm(data_shape)\n",
    "        self.ln2 = nn.LayerNorm(data_shape)\n",
    "        self.attention = nn.MultiheadAttention(embed_dim = 256, num_heads = 16, batch_first = True)\n",
    "        self.mlp = local_mlp(hidden_output = hidden_output_fnn1, dropout = dropout)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        self.x = self.gln1(data)\n",
    "        self.att_out, self.att_out_weights = self.attention(query = self.x,key = self.x,value = self.x)\n",
    "        #print(self.att_out.shape)\n",
    "        self.x_tilda = self.att_out + data\n",
    "        #print(self.x_tilda.shape)\n",
    "        self.x_second = self.ln2(self.x_tilda)\n",
    "        #self.x_second = self.mlp.forward(self.x_second)\n",
    "        self.dnn_output = self.mlp.forward(self.x_second)\n",
    "        self.x_second = self.dnn_output + self.x_tilda\n",
    "        \n",
    "        return self.x_second\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ca1b157",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visual_Transformer(nn.Module):\n",
    "    #embedding parameters, local encoder parameters\n",
    "    def __init__(self, x_amount = 7, y_amount = 7, x_con = 3518, y_con = 2800,\n",
    "              data_shape = (4,50,256), hidden_output_fnn = 1024, dropout = .5,\n",
    "              number_of_layers = 10):\n",
    "        super(Visual_Transformer, self).__init__()\n",
    "        self.embedding_block = embedding_block(x_amount = x_amount, y_amount = y_amount, x_con = x_con, y_con = y_con)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(number_of_layers):\n",
    "            self.blks.add_module(f'{i}', local_encoder_block(data_shape = data_shape))\n",
    "            \n",
    "    def forward(self,data):\n",
    "        self.x = self.embedding_block.forward(data)\n",
    "        print(self.x.shape)\n",
    "        i = 0 \n",
    "        for blk in self.blks:\n",
    "            print(f'This is {i} local attention run')\n",
    "            i += 1\n",
    "            self.x = blk(self.x) \n",
    "        return self.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1297c867",
   "metadata": {},
   "outputs": [],
   "source": [
    "class global_transformer(nn.Module):\n",
    "    def __init__(self, x_amount = 7, y_amount = 7, x_con = 3518, y_con = 2800,\n",
    "              data_shape = (4,50,256), hidden_output_fnn = 1024, dropout = .5,\n",
    "              number_of_layers = 10, num_layers_global = 10):\n",
    "        super(global_transformer,self).__init__()\n",
    "        self.data_shape = data_shape\n",
    "        new_data_shape = (1,data_shape[0]*data_shape[1],data_shape[2])\n",
    "        self.individual_transformer = Visual_Transformer(x_amount = x_amount, y_amount = y_amount, x_con = x_con,\n",
    "                                                   y_con = y_con, data_shape = data_shape, \n",
    "                                                    hidden_output_fnn = hidden_output_fnn, \n",
    "                                                    dropout = dropout, number_of_layers = number_of_layers)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers_global):\n",
    "            self.blks.add_module(f'{i}', global_encoder_block(data_shape = new_data_shape))\n",
    "            \n",
    "        self.flatten = nn.Flatten()\n",
    "            \n",
    "        self.class_head = classification_head(input_layer = data_shape[0]*data_shape[2], \n",
    "                                              hidden_output_class = 512, dropout = .5)\n",
    "    def forward(self, data):\n",
    "        self.x = self.individual_transformer.forward(data)\n",
    "        shape1, shape2, shape3 = self.x.shape\n",
    "        self.x = torch.reshape(self.x,(1, shape1 * shape2,shape3))\n",
    "        i = 0\n",
    "        for blk in self.blks:\n",
    "            print(f'This is {i} global attention run')\n",
    "            self.x = blk(self.x)\n",
    "            i += 1\n",
    "        \n",
    "        self.x = torch.squeeze(self.x)\n",
    "        print(self.x.shape)\n",
    "        self.x = self.x[[0, 1 * shape2, 2 * shape2, 3* shape2],:]\n",
    "        x = torch.reshape(self.x,(1,self.x.shape[0]*self.x.shape[1]))\n",
    "        print(x.shape)\n",
    "        x = self.class_head.forward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "591e8d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class classification_head(nn.Module):\n",
    "    def __init__(self, input_layer = 1024, hidden_output_class = 512, dropout = 0.5):\n",
    "        super(classification_head, self).__init__()\n",
    "        self.ln1 = nn.LayerNorm(input_layer)\n",
    "        self.fnn1 = nn.Linear(input_layer,hidden_output_class)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln2 = nn.LayerNorm(hidden_output_class)\n",
    "        self.fnn2 = nn.Linear(hidden_output_class,5)\n",
    "    def forward(self, data):\n",
    "        x = self.ln1(data)\n",
    "        x = self.fnn1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.ln2(x)\n",
    "        x = self.fnn2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4bb0dbf7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCC SHAPE:torch.Size([49, 500, 400])\n",
      "SHAPE OF THE TENSOR: torch.Size([49, 500, 400])\n",
      "SHAPE OF THE TENSOR: torch.Size([49, 500, 400])\n",
      "SHAPE OF THE TENSOR: torch.Size([49, 500, 400])\n",
      "SHAPE OF THE TENSOR: torch.Size([49, 500, 400])\n",
      "torch.Size([4, 50, 256])\n",
      "This is 0 local attention run\n",
      "This is 1 local attention run\n",
      "This is 2 local attention run\n",
      "This is 3 local attention run\n",
      "This is 4 local attention run\n",
      "This is 5 local attention run\n",
      "This is 6 local attention run\n",
      "This is 7 local attention run\n",
      "This is 8 local attention run\n",
      "This is 9 local attention run\n",
      "This is 0 global attention run\n",
      "This is 1 global attention run\n",
      "This is 2 global attention run\n",
      "This is 3 global attention run\n",
      "This is 4 global attention run\n",
      "This is 5 global attention run\n",
      "This is 6 global attention run\n",
      "This is 7 global attention run\n",
      "This is 8 global attention run\n",
      "This is 9 global attention run\n",
      "torch.Size([200, 256])\n",
      "torch.Size([1, 1024])\n",
      "The run time for one patient (4 images) is: 8.30808711051941 seconds\n",
      "The shape of output is: torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "x = global_transformer(x_con = 3500)\n",
    "output = x.forward(tensor)\n",
    "et = time.time()\n",
    "diff = et - st\n",
    "print(f'The run time for one patient (4 images) is: {diff} seconds')\n",
    "print(f'The shape of output is: {output.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f156a967",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2354,  2.3708,  0.8859,  ...,  5.0032,  1.1088, -0.4876],\n",
       "        [ 0.9578,  3.5941,  0.2396,  ...,  0.3077, -1.2816,  0.9832],\n",
       "        [-0.3766,  1.4778,  0.1023,  ...,  0.1568, -0.4792,  0.6722],\n",
       "        ...,\n",
       "        [ 0.7433,  1.7760, -0.2126,  ...,  1.7912,  0.4902,  1.4133],\n",
       "        [ 0.0743,  0.9737, -0.0658,  ..., -0.2920, -0.5391,  0.6376],\n",
       "        [-0.3415,  3.2997,  1.0004,  ...,  0.9011,  0.3884,  2.5579]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[1,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fac8757",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2354,  2.3708,  0.8859,  ...,  5.0032,  1.1088, -0.4876],\n",
       "        [ 0.9578,  3.5941,  0.2396,  ...,  0.3077, -1.2816,  0.9832],\n",
       "        [-0.3766,  1.4778,  0.1023,  ...,  0.1568, -0.4792,  0.6722],\n",
       "        ...,\n",
       "        [ 0.7433,  1.7760, -0.2126,  ...,  1.7912,  0.4902,  1.4133],\n",
       "        [ 0.0743,  0.9737, -0.0658,  ..., -0.2920, -0.5391,  0.6376],\n",
       "        [-0.3415,  3.2997,  1.0004,  ...,  0.9011,  0.3884,  2.5579]],\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.reshape(output,(200,256))[50:100,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca43b323",
   "metadata": {},
   "source": [
    "*****EDITED CODE FROM SAGEMAKER NOTEBOOK******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a325e8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2626, 0.8341, 0.8382, 0.7944, 0.2487])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand((5))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "95016701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(x,dim =-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b981d92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros(3500,2800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d0a51772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3500, 2800])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "31e90a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x[None,None,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aaa9edca",
   "metadata": {},
   "outputs": [],
   "source": [
    "cn = T.Resize((448,448))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e7fc113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([448, 448])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.squeeze(cn(x)).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
